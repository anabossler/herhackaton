# -*- coding: utf-8 -*-
"""geoalicante(wheread).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BeBk2bkdc2roDbBfZYcU05BUuFwx5CMi
"""

#Para conectar con google drive
from google.colab import drive 
drive.mount('/content/gdrive')

#Manipulación de datos 
import numpy as np
import pandas as pd
# Graficar
import matplotlib.pyplot as plt
import seaborn as sns
# Geoespacial
import folium
import geopy
# Machine Learning
from sklearn import preprocessing, cluster
import scipy
# Deep Learning
#import minisom # no carga minisom

Base = pd.read_csv('/content/eda alicante - Página1 copia.csv', decimal=',')

# Función para eliminar columnas que no usaremos 
def Drop (data,colum_reng,ax):
  #data es el data frame 
  #column:rengo es el arreglo de las columnas o de los renglones que queremos eliminar
  #ax es si queremos borrar renglon o columna
  #si ax =0, entonces es renglon, si ax=1, entonces es columna
  #Columnas es el arreglo de nombre de columnas que no queremos  
  for dato in colum_reng : 
    data.drop(dato,axis=ax,inplace=True)
  
  return(data)

#Quitamos renglones que no funciona :
BaseN = Base[0:28]
#BaseN

#Quitamos las columnas que no nos sirven 
BaseN.columns

NoCol =['Total', 'Total men', 'Total Spanish',
       'Spanish men', 'Total Foreign born', 'Foreign men',
        'Distance nearest hospital km', 
       'Transport line', 'Distance nearest public hospital', 'porcentage',
       'Zipcode']
BaseBuena = Drop(BaseN,NoCol,1) #Quitamos las columnas no deseadas
BaseBuena

BaseBuena.to_csv('BaseLonyLat.csv')

BaseBuena.info()

#Vemos que columnas nos quedan
BaseBuena.columns

BaseBuena.info()
#A dirección le faltan 2 datos
#A paises que exporta le faltan dos datos 
#Que renglones son ?

#Quitamos los renglones no deseados 
Indice =[165]
BaseBN = Drop(BaseBuena,Indice,0)
BaseBN.info() #Ahora ya todos tienen la misma cantidad de objetos no nulos

#Funcion para separar oración por espacios en un arreglo y obtener la dirección del estado

def SeparadorDireccion (Base,columna) : 
  #Base es el nombre de la base de datos
  #Columna es nombre la columna del data frame que queremos que sus entradas se
  #separen por arreglos de palabras
  #Columna es un string 
  #Arreglo es aquel que será el clasificador de dicha columna 

  Nueva_Col=[]
  n = len(Base[columna]) #Longitud de la columna 
  #print(n)
  for i in range(n):
    #print(i)
    col=Base[columna][i] #Renglon i 
    arr = col.split() # Separamos el renglon en arreglo por espacios 
    #print(i,arr)
    if '-' in arr : 
      index= arr.index('-')
      Nueva_Col.append(arr[index+1])

  return(Nueva_Col)

b = BaseBNF['Dirección'][165]
b
#arr = b.split()
#arr

Dir = SeparadorDireccion(BaseBNF,'Dirección') #Direcciones
#len(Dir= #Me da dir con longitud 263 :D
BaseFin = pd.DataFrame()
#BaseFin['Bodega']=BaseBNF['Bodega']
BaseFin['City']= Dir
BaseFin['Street Address']=BaseBNF['Dirección']
BaseFin['Latitude']=BaseBNF['Latitude']
BaseFin['Longitude']=BaseBNF['Longitude']
BaseFin.tail(20)

dtf=BaseFin

#Seleccionamos solo las bodegas que están en Madrid y lo guardamos en la base de datos

BaseBuena = BaseBuena[BaseBuena[["City","Street Address","Longitude","Latitude"]].reset_index(drop=True)
dtf = dtf.reset_index().rename(columns={"index":"id"})
dtf

dtf.info()

#Simulamos los valores de Potential, Staff, Capacity y Cost
#¿Pod´riamos obtener estos datos de nuestras bases de datos? 
#Ponential podría ser almacenaciento
#Staff cuando almacenamiento se esta usando
#Capacidad se define igual y costo también 
BaseBuena["Potential"] = np.random.randint(low=3, high=10+1, size=len(BaseBuena))
BaseBuena["Staff"] = BaseBuena["Potential"].apply(lambda x: int(np.random.rand()*x)+1)
BaseBuena["Capacity"] = BaseBuena["Potential"] - BaseBuena["Staff"]
BaseBuena["Cost"] = np.random.choice(["high","medium","low"], size=len(BaseBuena), p=[0.4,0.5,0.1])
BaseBuena.head(24)

x = "Cost"
ax = dtf[x].value_counts().sort_values().plot(kind="barh")
totals = []
for i in ax.patches:
    totals.append(i.get_width())
total = sum(totals)
for i in ax.patches:
     ax.text(i.get_width()+.3, i.get_y()+.20, 
     str(round((i.get_width()/total)*100, 2))+'%', 
     fontsize=10, color='black')
ax.grid(axis="x")
plt.suptitle(x, fontsize=20)
plt.show()

city = "ALICANTE"
## get location
locator = geopy.geocoders.Nominatim(user_agent="MyCoder")
location = locator.geocode(city)
print(location)
## keep latitude and longitude only
location = [location.latitude, location.longitude]
print("[lat, long]:", location) #Obtenemos la latitud y longitud de la ciudad

import sys
print(sys.getrecursionlimit())
sys.setrecursionlimit(1500)

x = "Cost"
ax = BaseBuena[x].value_counts().sort_values().plot(kind="barh")
totals = []
for i in ax.patches:
    totals.append(i.get_width())
total = sum(totals)
for i in ax.patches:
     ax.text(i.get_width()+.3, i.get_y()+.20, 
     str(round((i.get_width()/total)*100, 2))+'%', 
     fontsize=10, color='black')
ax.grid(axis="x")
plt.suptitle(x, fontsize=20)
plt.show()

X =BaseBuena[["Latitude","Longitude"]]
max_k = 10
## iterations
distortions = [] 
for i in range(1, max_k+1):
    if len(X) >= i:
       model = cluster.KMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=10, random_state=0)
       model.fit(X)
       distortions.append(model.inertia_)
## best k: the lowest derivative
k = [i*100 for i in np.diff(distortions,2)].index(min([i*100 for i 
     in np.diff(distortions,2)]))
## plot
fig, ax = plt.subplots()
ax.plot(range(1, len(distortions)+1), distortions)
ax.axvline(k, ls='--', color="red", label="k = "+str(k))
ax.set(title='The Elbow Method', xlabel='Number of clusters', 
       ylabel="Distortion")
ax.legend()
ax.grid(True)
plt.show()

k = 5
model = cluster.KMeans(n_clusters=k, init='k-means++')
X = BaseBuena[["Latitude","Longitude"]]
## clustering
base_X = X.copy()
base_X["cluster"] = model.fit_predict(X)
## find real centroids
closest, distances = scipy.cluster.vq.vq(model.cluster_centers_, 
                     base_X.drop("cluster", axis=1).values)
base_X["centroids"] = 0
for i in closest:
    base_X["centroids"].iloc[i] = 1
## add clustering info to the original dataset
BaseBuena[["cluster","centroids"]] = base_X[["cluster","centroids"]]
BaseBuena.sample(5)

## plot
fig, ax = plt.subplots()
sns.scatterplot(x="Latitude", y="Longitude", data=BaseBuena, 
                palette=sns.color_palette("bright",k),
                hue='cluster', size="centroids", size_order=[1,0],
                legend="brief", ax=ax).set_title('Clustering + (k='+str(k)+')')
th_centroids = model.cluster_centers_
ax.scatter(th_centroids[:,0], th_centroids[:,1], s=50, c='black', 
           marker="x")

model = cluster.AffinityPropagation()

k = BaseBuena["cluster"].nunique()
sns.scatterplot(x="Latitude", y="Longitude", data=BaseBuena, 
                palette=sns.color_palette("bright",k),
                hue='cluster', size="centroids", size_order=[1,0],
                legend="brief").set_title('Clustering + (k='+str(k)+')')

new = pd.DataFrame()
for c in sorted(BaseBuena["cluster"].unique()):
    new_cluster = BaseBuena[BaseBuena["cluster"]==c]
    
    ## hubs and targets
    lst_hubs = new_cluster[new_cluster["Cost"]=="low"
               ].sort_values("Capacity").to_dict("records")
    lst_targets = new_cluster[new_cluster["Cost"]=="high"
               ].sort_values("Staff").to_dict("records")
    ## move targets
    for target in lst_targets:
         for hub in lst_hubs:
             ### if hub has space
             if hub["Capacity"] > 0:
                residuals = hub["Capacity"] + target["Staff"]
                #### case of hub has still capacity: do next target
                if residuals >= 0:
                   hub["Staff"] += target["Staff"]
                   hub["Capacity"] = hub["Potential"] + hub["Staff"]
                   target["Capacity"] = target["Potential"]
                   target["Staff"] = 0
                   break
                #### case of hub is full: do next hub
                else:
                   hub["Capacity"] = 0
                   hub["Staff"] = hub["Potential"]
                   target["Staff"] = -residuals
                   target["Capacity"] = target["Potential"] + target["Staff"]
    new = new.append(pd.DataFrame(lst_hubs)
                 ).append(pd.DataFrame(lst_targets))

    new = new.append(BaseBuena[BaseBuena["Cost"]=="medium"]
                 ).reset_index(drop=True).sort_values(
                 ["cluster","Staff"])
new.head()

new["trial site"] = new["Staff"].apply(lambda x: 1 
                                           if x==0 else 0)
print("trial site:", new['trial site'].sum())
print(new[['hospitalcode','cluster','trial site']])

new

new.to_csv('hospitals.csv')